{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4273\n"
     ]
    }
   ],
   "source": [
    "code = \"\"\"\n",
    "import os\n",
    "import pickle\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "from langchain_core.documents.base import Document\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def semantic_code_splitter(root_path, embedding_max_length=8191):\n",
    "    # Average length of string per token = 4.35\n",
    "\n",
    "    document_path = root_path\n",
    "    loader = DirectoryLoader(\n",
    "        document_path,\n",
    "        glob=[\"*.md\", \"*.py\", \"*.ts\", \"*.js\", \"*.tsx\", \"*.jsx\"],\n",
    "        loader_cls=TextLoader,\n",
    "        recursive=True,\n",
    "    )\n",
    "    documents = loader.load()\n",
    "\n",
    "    if not documents:\n",
    "        print(\"No documents found in the specified directory.\")\n",
    "        raise Exception(\"No documents found in the specified directory.\")\n",
    "\n",
    "    # Seperators: ['\\nclass ', '\\ndef ', '\\n\\tdef ', '\\n\\n', '\\n', ' ', '']\n",
    "    python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language=Language.PYTHON,\n",
    "        chunk_size=200,\n",
    "        chunk_overlap=0,\n",
    "        keep_separator=True,\n",
    "        strip_whitespace=False,\n",
    "    )\n",
    "    documents = python_splitter.split_documents(documents)\n",
    "\n",
    "    openAIEmbedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "    print(\"Embedding documents for semantic splitter...\")\n",
    "    embeddings = openAIEmbedding.embed_documents(\n",
    "        [document.page_content for document in documents]\n",
    "    )\n",
    "\n",
    "    # group documents with the same metadata source value and merge embeddings\n",
    "    documents_grouped_by_source = {}\n",
    "    for doc, embedding in zip(documents, embeddings):\n",
    "        source = doc.metadata[\"source\"]\n",
    "        if source not in documents_grouped_by_source:\n",
    "            documents_grouped_by_source[source] = []\n",
    "        documents_grouped_by_source[source].append(\n",
    "            {\"content\": doc.page_content, \"embedding\": embedding}\n",
    "        )\n",
    "\n",
    "    print(\"Calculating semantic similarities...\")\n",
    "    cosine_similarities = []\n",
    "    for source, docs_in_same_source in documents_grouped_by_source.items():\n",
    "        cosine_similarities_grouped_by_source = {\"source\": source, \"similarities\": []}\n",
    "        for i in range(len(docs_in_same_source) - 1):\n",
    "            cosine_similarity_result = cosine_similarity(\n",
    "                [docs_in_same_source[i][\"embedding\"]],\n",
    "                [docs_in_same_source[i + 1][\"embedding\"]],\n",
    "            )\n",
    "            cosine_similarities_grouped_by_source[\"similarities\"].append(\n",
    "                cosine_similarity_result[0][0]\n",
    "            )\n",
    "        cosine_similarities.append(cosine_similarities_grouped_by_source)\n",
    "\n",
    "    print(\"Merging semantically similar documents...\")\n",
    "    documents_after_semantic_merging = []\n",
    "    for (source, document), cosine_similarities_grouped_by_source in zip(\n",
    "        documents_grouped_by_source.items(), cosine_similarities\n",
    "    ):\n",
    "        if source != cosine_similarities_grouped_by_source[\"source\"]:\n",
    "            print(\n",
    "                f\"Source mismatch: {source} vs {cosine_similarities_grouped_by_source['source']}\"\n",
    "            )\n",
    "            raise Exception(\"Source mismatch\")\n",
    "\n",
    "        merged_chunk = \"\"\n",
    "        for idx, similarity in enumerate(\n",
    "            cosine_similarities_grouped_by_source[\"similarities\"]\n",
    "        ):\n",
    "            # print(f\"Similarity between {idx} and {idx+1}: {similarity}\")\n",
    "            # print(\"sentence 1: \", document[idx][\"content\"])\n",
    "            # print(\"sentence 2: \", document[idx+1][\"content\"])\n",
    "            # print(\"-------------------------------------------------\")\n",
    "            if similarity > 0.7:\n",
    "                if len(merged_chunk) == 0:\n",
    "                    merged_chunk = (\n",
    "                        document[idx][\"content\"] + document[idx + 1][\"content\"]\n",
    "                    )\n",
    "                if len(merged_chunk) > embedding_max_length:\n",
    "                    documents_after_semantic_merging.append(\n",
    "                        Document(merged_chunk, metadata={\"source\": source})\n",
    "                    )\n",
    "                    merged_chunk = \"\"\n",
    "                else:\n",
    "                    merged_chunk += document[idx + 1][\"content\"]\n",
    "            else:\n",
    "                documents_after_semantic_merging.append(\n",
    "                    Document(merged_chunk, metadata={\"source\": source})\n",
    "                )\n",
    "\n",
    "    return documents_after_semantic_merging\n",
    "\"\"\"\n",
    "print(len(code))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
