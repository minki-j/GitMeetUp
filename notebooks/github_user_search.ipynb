{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch users based on location\n",
    "\n",
    "Due to the Github API's limit of returning only the first 1000 items, we must divide the query into intervals based on the account creation dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def generate_date_intervals(\n",
    "    interval_days,\n",
    "    start_year=2008,\n",
    "    end_date=datetime.now(),\n",
    "):\n",
    "    dates = []\n",
    "    end_year = end_date.year\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for month in range(1, 13):\n",
    "            # Skip months beyond the end date in the final year\n",
    "            if year == end_year and month > end_date.month:\n",
    "                break\n",
    "\n",
    "            # Calculate the number of days in the current month\n",
    "            if month == 12:\n",
    "                days_in_month = (\n",
    "                    datetime(year + 1, 1, 1) - datetime(year, month, 1)\n",
    "                ).days\n",
    "            else:\n",
    "                days_in_month = (\n",
    "                    datetime(year, month + 1, 1) - datetime(year, month, 1)\n",
    "                ).days\n",
    "\n",
    "            # Adjust the days in the month if it's the current month and year\n",
    "            if year == end_date.year and month == end_date.month:\n",
    "                days_in_month = end_date.day\n",
    "\n",
    "            # Loop through the month in intervals of `interval_days`\n",
    "            for day in range(1, days_in_month + 1, interval_days):\n",
    "                start_date = datetime(year, month, day)\n",
    "                # Ensure the end date does not exceed the month or the specified end date\n",
    "                end_interval_date = min(\n",
    "                    start_date + timedelta(days=interval_days - 1),\n",
    "                    datetime(year, month, days_in_month) + timedelta(days=1),\n",
    "                    end_date\n",
    "                    + timedelta(days=1),  # Ensure we don't go beyond the end_date\n",
    "                )\n",
    "\n",
    "                # Format the dates as strings and add them to the list\n",
    "                dates.append(\n",
    "                    f\"{start_date.strftime('%Y-%m-%d')}..{end_interval_date.strftime('%Y-%m-%d')}\"\n",
    "                )\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "def get_page_num(url):\n",
    "    query_string = urlparse(url).query\n",
    "    params = parse_qs(query_string)\n",
    "    return params.get(\"page\", [None])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "token = os.getenv(\"GITHUB_TOKEN\")\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "base_url = \"https://api.github.com/search/users?\"\n",
    "location = \"montreal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_users_from_github(location, dates, base_url, headers):\n",
    "    total_users = []\n",
    "    log = []\n",
    "\n",
    "    for idx, date in enumerate(dates):\n",
    "        # if idx > 0:\n",
    "        #     print(\"DEBUG MODE: breaking after 1 iterations\")\n",
    "        #     print(\n",
    "        #         f\"Processed {idx -1} {dates[idx - 1]} out of {len(dates)} / Collected users: {len(users)} Total users: {len(total_users)}\"\n",
    "        #     )\n",
    "        #     return users\n",
    "        log_for_date = {\"date\": date, \"messages\": [], \"overflow\": False}\n",
    "        next_url = \"first_page\"\n",
    "        while True:\n",
    "            if next_url is not None and next_url != \"first_page\":\n",
    "                response = requests.get(next_url, headers=headers)\n",
    "            elif next_url == \"first_page\":\n",
    "                response = requests.get(\n",
    "                    base_url,\n",
    "                    headers=headers,\n",
    "                    params={\n",
    "                        \"q\": f\"location:{location} created:{date}\",\n",
    "                        \"page\": 1,\n",
    "                        \"per_page\": 100,\n",
    "                        \"sort\": \"joined\",\n",
    "                        \"order\": \"desc\",\n",
    "                    },\n",
    "                )\n",
    "                last_url = response.links.get(\"last\", {}).get(\"url\")\n",
    "                last_page_num = get_page_num(last_url)\n",
    "                if last_page_num == 10:\n",
    "                    log_for_date[\"overflow\"] = True\n",
    "\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                users = response.json()[\"items\"]\n",
    "                if users:\n",
    "                    total_users.extend(users)\n",
    "                    log_for_date[\"messages\"].append(f\"{len(users)} total user added\")\n",
    "                else:\n",
    "                    if next_url != \"first_page\":\n",
    "                        log_for_date[\"messages\"].append(f\"NO USER in: {next_url}\")\n",
    "                next_url = response.links.get(\"next\", {}).get(\"url\")\n",
    "            else:\n",
    "                print(f\"Failed to fetch repositories: {response.status_code}\")\n",
    "                print(response)\n",
    "                print(\"Waiting for 60 seconds\")\n",
    "                time.sleep(60)\n",
    "\n",
    "            log.extend(log_for_date)\n",
    "\n",
    "            if response.headers.get(\"X-RateLimit-Remaining\") == \"0\":\n",
    "                # print(\"Rate limit reached. Waiting for 60 seconds\")\n",
    "                print(\n",
    "                    f\"Processed {idx + 1} out of {len(dates)} / Total users: {len(total_users)}\"\n",
    "                )\n",
    "\n",
    "                # Save the list of users to a file\n",
    "                with open(f\"../data/users_in_{location}.json\", \"w\") as file:\n",
    "                    json.dump(total_users, file, indent=4)\n",
    "\n",
    "                # Save the log to a file\n",
    "                with open(f\"../data/log_{location}.json\", \"w\") as file:\n",
    "                    json.dump(log, file, indent=4)\n",
    "\n",
    "                time.sleep(60)\n",
    "\n",
    "    return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = generate_date_intervals(interval_days=30)\n",
    "users = fetch_users_from_github(location, dates, base_url, headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deduplicate users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total users: 4614\n",
      "Total unique users: 4495\n",
      "Total duplicated users: 119\n",
      "Total unique users after deduplication: 4495\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "location = \"montreal\"\n",
    "\n",
    "with open(f\"../data/users_in_{location}.json\") as file:\n",
    "    users = json.load(file)\n",
    "\n",
    "user_ids = [user[\"id\"] for user in users]\n",
    "print(f\"Total users: {len(users)}\")\n",
    "print(f\"Total unique users: {len(set(user_ids))}\")\n",
    "\n",
    "if len(users) != len(set(user_ids)):\n",
    "    duplicated_user_ids = []\n",
    "    for user_id in set(user_ids):\n",
    "        if user_ids.count(user_id) > 1:\n",
    "            duplicated_user_ids.append(user_id)\n",
    "    print(f\"Total duplicated users: {len(duplicated_user_ids)}\")\n",
    "\n",
    "    # remove duplicated users\n",
    "    if len(duplicated_user_ids) > 0:\n",
    "        deduplicated_users = []\n",
    "        seen_user_ids = set()\n",
    "\n",
    "        for user in users:\n",
    "            # If the user's ID hasn't been seen, add the user to the deduplicated list and mark the ID as seen\n",
    "            # This will keep the order of the users list\n",
    "            if user[\"id\"] not in seen_user_ids:\n",
    "                deduplicated_users.append(user)\n",
    "                seen_user_ids.add(user[\"id\"])\n",
    "\n",
    "        with open(f\"../data/users_in_{location}_deduplicated222.json\", \"w\") as file:\n",
    "            json.dump(deduplicated_users, file, indent=4)\n",
    "\n",
    "        print(f\"Total unique users after deduplication: {len(deduplicated_users)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
